# -*- coding: utf-8 -*-
"""DSC599.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qyj7GDYfyVWTbN_MsdI7ZjFR-oimJA-B
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

#1. Load and split data
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

#2. Define models
log_clf = LogisticRegression(max_iter=1000)
svm_clf = SVC(probability=True, kernel='linear')
tree_clf = DecisionTreeClassifier(max_depth=5, random_state=42)

#3. Create ensemble
ensemble_clf = VotingClassifier(
    estimators=[
        ('LogReg', log_clf),
        ('SVM', svm_clf),
        ('Tree', tree_clf)
    ],
    voting='soft'
)

#4. Train all models
models = {
    'Logistic Regression': log_clf,
    'SVM': svm_clf,
    'Decision Tree': tree_clf,
    'Ensemble': ensemble_clf
}

for name, model in models.items():
    model.fit(X_train, y_train)

#5. Evaluate
accuracies = {name: accuracy_score(y_test, model.predict(X_test))
              for name, model in models.items()}

#6. Print model performance
print("Model Accuracies:")
for name, acc in accuracies.items():
    print(f"  {name}: {acc:.3f}")

#7. Bar chart visualization
plt.figure(figsize=(8, 5))
plt.bar(accuracies.keys(), accuracies.values(), color=['skyblue', 'lightgreen', 'salmon', 'gold'])
plt.title('Model Comparison â€” Individual vs Ensemble')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# 8.2D Decision Boundary visualization
# (For simplicity, only use first two features)
X_2D = X[:, :2]
X_train2D, X_test2D, y_train2D, y_test2D = train_test_split(X_2D, y, test_size=0.3, random_state=39)

ensemble_2D = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression(max_iter=1000)),
        ('svm', SVC(probability=True, kernel='linear')),
        ('tree', DecisionTreeClassifier(max_depth=5))
    ],
    voting='soft'
)
ensemble_2D.fit(X_train2D, y_train2D)

# Plot decision boundary
x_min, x_max = X_2D[:, 0].min() - 1, X_2D[:, 0].max() + 1
y_min, y_max = X_2D[:, 1].min() - 1, X_2D[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = ensemble_2D.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X_test2D[:, 0], X_test2D[:, 1], c=y_test2D, edgecolors='k')
plt.title('Ensemble Model Decision Boundary (2D Projection)')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()